{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "mental-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "prescribed-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    list = []\n",
    "    for f in glob.glob(\"*.json\"):   #loading only json files\n",
    "        with open(f, \"rb\") as infile:\n",
    "            list += (json.load(infile))     #combing all the data to a big list\n",
    "    return pd.DataFrame(list)       #convert the list to a pandas datarame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "golden-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproccing the data orgnize, clean, remove nans etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "statewide-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_old_tweets(df):\n",
    "    print(\"Dataset and tweets*before* preprocessing and text normalization:\\n\", df.text.head(5))\n",
    "    df.info()\n",
    "    df['date'] = pd.to_datetime(df['created_at'], errors='coerce')  # changing data to pd.datatime\n",
    "    df['day'] = df['date'].dt.day_of_week  # taking the day of tweet as a feature\n",
    "    df['hour'] = df['date'].dt.hour  # taking the hour of tweet as a feature\n",
    "    entities = pd.json_normalize(df['entities'])  # Changing dictionary to columns\n",
    "    user = pd.json_normalize(df['user'])\n",
    "    user_ent = user['entities.url.urls'].apply(pd.Series)\n",
    "    user_entities_url = pd.json_normalize(user_ent[0].fillna(method='pad'))\n",
    "    pdList = [df, user, entities, user_entities_url]\n",
    "    df = pd.concat(pdList, axis=1)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df.drop(['date', 'created_at', 'user', 'entities', 'entities.url.urls', 'indices'], axis=1, inplace=True)\n",
    "    df = df.loc[:, df.isnull().mean() < 0.4]  # Removing features with too much missing values\n",
    "    for col in df.columns:\n",
    "        if type(df[col][0]) == list and df[col].astype(\n",
    "                bool).mean() < 0.7:  # check if there is a list and if that list has only repeated values\n",
    "            df.drop([col], axis=1, inplace=True)\n",
    "    df = df.loc[:,\n",
    "         df.nunique() != len(df)]  # Removing features that have the have different value for each data points\n",
    "    df = df.loc[:, df.nunique() != 1]  # Removing features that have only one value\n",
    "    df = df.applymap(lambda s: s.lower() if type(s) == str else s)  # lower case all the text\n",
    "    df['source'] = np.where(df['source'].str.contains(\"android\"), 2,\n",
    "                            (np.where(df['source'].str.contains(\"iphone\"), 1, 0)))  # chaning values to numbers\n",
    "    tqdm.pandas(desc='clean_old_tweets_progress')  # giving a progress bar for the next (slow) function\n",
    "    df['text'] = df['text'].progress_apply(clean_text)  # clean text\n",
    "    print(\"\\nDataset and tweets *after* preprocessing and text normalization:\\n\", df.text.head(5))\n",
    "    df.info()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "numerous-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearning the text by tokening the words, removing all non letter characters, stop words and using lemmaization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "extended-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokenzier = nltk.tokenize.casual.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    text = tokenzier.tokenize(text)\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "    text = [word for word in text if not word in stopwords.words(\"english\")]\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    text = [lemma.lemmatize(w, pos=\"v\") for w in text]  #for verbs\n",
    "    return [lemma.lemmatize(w, pos=\"n\") for w in text]  #for nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "suffering-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # choosing only android (1) and iphone(0) devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "alive-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cell_phones(x, y):\n",
    "    df = pd.concat([x, y], axis=1)\n",
    "    x_cells = df[df['source'] > 0]      # only cell phones\n",
    "    x_other = df[df['source'] == 0]     # other_devices\n",
    "    y_cells = x_cells.source\n",
    "    y_other = x_other.source\n",
    "    x_cells = x_cells.drop(['source'], axis=1)\n",
    "    x_other = x_other.drop(['source'], axis=1)\n",
    "    return x_cells, y_cells, x_other, y_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "introductory-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # checking if the data is unbalance and fixing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "covered-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalance_data(X, y):\n",
    "    im1 = sns.countplot(y)\n",
    "    im1.set_xticklabels(['iPhone', 'Android'])\n",
    "    plt.show()\n",
    "    under_sample = RandomUnderSampler(random_state=0)\n",
    "    X_resampled, y_resampled = under_sample.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "educated-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Creating a dataframe by fitting the data and the tfidf (on the text column) so it will run on the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "indonesian-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(df):\n",
    "    y = df.source\n",
    "    X = df.drop(['source'], axis=1)\n",
    "    X_oh = pd.get_dummies(X[X.columns.difference(['text'])], drop_first=True) # one hot vectors on all the data except the text and source (labels)\n",
    "    X_oh.insert(0, \"text\", X.text)\n",
    "    X_oh.text = [\" \".join(tokens) for tokens in X_oh.text.values] #changing the lists to strings\n",
    "    x_cells, y_cells, x_other, y_other = filter_cell_phones(X_oh, y)\n",
    "    X_balanced, y_balanced = imbalance_data(x_cells, y_cells)\n",
    "    tfv = TfidfVectorizer(min_df=3, max_features=None,\n",
    "                          strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                          stop_words='english')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.25, random_state=0)\n",
    "    tfv.fit(X_train['text'])\n",
    "    X_train, train_sm = sparse_matrix_fit(X_train, tfv)\n",
    "    X_test, test_sm = sparse_matrix_fit(X_test, tfv)\n",
    "    x_other, _ = sparse_matrix_fit(x_other, tfv)\n",
    "    \n",
    "    #svd = TruncatedSVD(n_components=500, n_iter=7, random_state=0)     #reducing the number of features to 500 so it wont have more features than data points\n",
    "    #X = svd.fit_transform(X)   # it reduced the AUC by 2% so there was no point.\n",
    "    return X_train, X_test, y_train, y_test, x_other, y_other, tfv, train_sm, test_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "recent-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating and fitting the sparse matrix to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "respective-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_matrix_fit(X, tfv):\n",
    "    sparse_mat = pd.DataFrame.sparse.from_spmatrix(tfv.transform(X['text']))  # make the sparse matrix fit the pandas df\n",
    "    X = pd.concat([X.reset_index(), sparse_mat.reset_index()], axis=1).drop(['text', 'index'], axis=1)\n",
    "    X.columns = [str(i) for i in range(0, (X.shape[1]))]  # changing columns names so it two columns wont share a name\n",
    "    return X, sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "union-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and fitting xgboost model classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "covered-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(X_train, X_test, y_train, y_test):\n",
    "    xgboost = xgb.XGBClassifier(nthread=-1, eval_metric='auc', random_state=0, enable_categorical=True)\n",
    "    xgboost.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "    y_pred = xgboost.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)*100\n",
    "    return xgboost, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "commercial-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of 2015-2016 tweets with xgboost model fitted on all the meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "enormous-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2016(x_other, xgboost):\n",
    "    y_pred = pred_model(x_other, xgboost)\n",
    "    print(f'The classifier found that Trump wrote {y_pred.value_counts().values[0]} tweets from the other devices'\n",
    "          f' while the unknown assistant wrote {y_pred.value_counts().values[1]} tweets from the other devices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "severe-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction according to xgb model and transform back to pd series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "future-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_model(x, xgboost):\n",
    "    return pd.Series(xgboost.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "living-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess of 2018 tweets, I found that the data features have been changed, thus the model can not \n",
    "# be used, only a model that was fitted on the text of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "postal-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2018_tweets(df):\n",
    "    print(\"Dataset and tweets*before* preprocessing and text normalization:\\n\", df.text.head(5))\n",
    "    df.info()\n",
    "    df.text = df.text.fillna(df.full_text)\n",
    "    df['date'] = pd.to_datetime(df['created_at'], errors='coerce') #changing data to pd.datatime\n",
    "    df['day'] = df['date'].dt.day_of_week   #taking the day of tweet as a feature\n",
    "    df['hour'] = df['date'].dt.hour     #taking the hour of tweet as a feature\n",
    "    entities = pd.json_normalize(df['entities'])\n",
    "    user = pd.json_normalize(df['user'])\n",
    "    pdList = [df, user, entities]\n",
    "    df = pd.concat(pdList, axis=1)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df.drop(['date', 'created_at', 'user', 'entities', 'full_text'], axis=1, inplace=True)\n",
    "    df = df.loc[:, df.isnull().mean() < 0.4]    # Removing features with too much missing values\n",
    "    for col in df.columns:\n",
    "        if type(df[col][0]) == list and df[col].astype(bool).mean() < 0.7:\n",
    "            df.drop([col], axis=1, inplace=True)\n",
    "    df = df.loc[:, df.nunique() != len(df)]     #Removing features that have the have different value for each data points\n",
    "    df = df.loc[:, df.nunique() != 1]       #Removing features that have only one value\n",
    "    df = df.applymap(lambda s: s.lower() if type(s) == str else s)      #lower case all the text\n",
    "    df['source'] = np.where(df['source'].str.contains(\"android\"), 2, (np.where(df['source'].str.contains(\"iphone\"), 1, 0))) #chaning values to numbers\n",
    "    tqdm.pandas(desc='clean_text_progress')     #giving a progress bar for the next (slow) function\n",
    "    df['text'] = df['text'].progress_apply(clean_text) # clean text\n",
    "    print(\"\\nDataset and tweets *after* preprocessing and text normalization:\\n\", df.text.head(5))\n",
    "    df.info()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "under-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for 2018 tweets according to the xgb text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "spare-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2018(df, xgboost, tfv):\n",
    "    text = [\" \".join(tokens) for tokens in df['text'].values] #changing the lists to strings\n",
    "    sparse_mat = pd.DataFrame.sparse.from_spmatrix(tfv.transform(text))\n",
    "    sparse_mat.columns = [str(i) for i in range(0, (sparse_mat.shape[1]))]\n",
    "    y_pred = pred_model(sparse_mat, xgboost_text_model)\n",
    "    print(f'The classifier found that Trump wrote {y_pred.value_counts().values[0]} tweets in 2018 '\n",
    "          f' while the unknown assistant wrote {y_pred.value_counts().values[1]} tweets in 2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "adjusted-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and tweets*before* preprocessing and text normalization:\n",
      " 0    I would like to wish everyone A HAPPY AND HEAL...\n",
      "1    Do you believe that The State Department, on N...\n",
      "2    THANK YOU ILLINOIS! Let's not forget to get fa...\n",
      "3    HAPPY BIRTHDAY to my son, @DonaldJTrumpJr! Ver...\n",
      "4    I would feel sorry for @JebBush and how badly ...\n",
      "Name: text, dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11761 entries, 0 to 11760\n",
      "Data columns (total 33 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   contributors               0 non-null      object \n",
      " 1   truncated                  11761 non-null  bool   \n",
      " 2   text                       11761 non-null  object \n",
      " 3   is_quote_status            11761 non-null  bool   \n",
      " 4   in_reply_to_status_id      18 non-null     float64\n",
      " 5   id                         11761 non-null  int64  \n",
      " 6   favorite_count             11761 non-null  int64  \n",
      " 7   source                     11761 non-null  object \n",
      " 8   retweeted                  11761 non-null  bool   \n",
      " 9   coordinates                1135 non-null   object \n",
      " 10  entities                   11761 non-null  object \n",
      " 11  in_reply_to_screen_name    38 non-null     object \n",
      " 12  id_str                     11761 non-null  object \n",
      " 13  retweet_count              11761 non-null  int64  \n",
      " 14  in_reply_to_user_id        38 non-null     float64\n",
      " 15  favorited                  11761 non-null  bool   \n",
      " 16  user                       11761 non-null  object \n",
      " 17  geo                        1135 non-null   object \n",
      " 18  in_reply_to_user_id_str    38 non-null     object \n",
      " 19  lang                       11761 non-null  object \n",
      " 20  created_at                 11761 non-null  object \n",
      " 21  in_reply_to_status_id_str  18 non-null     object \n",
      " 22  place                      1654 non-null   object \n",
      " 23  possibly_sensitive         3489 non-null   object \n",
      " 24  extended_entities          1250 non-null   object \n",
      " 25  quoted_status_id           220 non-null    float64\n",
      " 26  quoted_status              215 non-null    object \n",
      " 27  quoted_status_id_str       220 non-null    object \n",
      " 28  retweeted_status           188 non-null    object \n",
      " 29  scopes                     63 non-null     object \n",
      " 30  withheld_scope             1 non-null      object \n",
      " 31  withheld_in_countries      1 non-null      object \n",
      " 32  withheld_copyright         1 non-null      object \n",
      "dtypes: bool(4), float64(3), int64(3), object(23)\n",
      "memory usage: 2.6+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clean_old_tweets_progress: 100%|████████████████████████████████████████████████| 11761/11761 [00:31<00:00, 370.15it/s]\n",
      "C:\\Users\\tzach\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset and tweets *after* preprocessing and text normalization:\n",
      " 0    [would, like, wish, everyone, happy, healthy, ...\n",
      "1    [believe, state, department, new, eve, release...\n",
      "2    [thank, illinois, forget, get, family, friend,...\n",
      "3                        [happy, birthday, son, proud]\n",
      "4    [would, feel, sorry, badly, campaign, fact, ta...\n",
      "Name: text, dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11761 entries, 0 to 11760\n",
      "Data columns (total 40 columns):\n",
      " #   Column                              Non-Null Count  Dtype \n",
      "---  ------                              --------------  ----- \n",
      " 0   truncated                           11761 non-null  bool  \n",
      " 1   text                                11761 non-null  object\n",
      " 2   is_quote_status                     11761 non-null  bool  \n",
      " 3   favorite_count                      11761 non-null  int64 \n",
      " 4   source                              11761 non-null  int32 \n",
      " 5   retweeted                           11761 non-null  bool  \n",
      " 6   retweet_count                       11761 non-null  int64 \n",
      " 7   lang                                11761 non-null  object\n",
      " 8   day                                 11761 non-null  int64 \n",
      " 9   hour                                11761 non-null  int64 \n",
      " 10  geo_enabled                         11761 non-null  bool  \n",
      " 11  description                         11761 non-null  object\n",
      " 12  profile_image_url_https             11761 non-null  object\n",
      " 13  profile_sidebar_fill_color          11761 non-null  object\n",
      " 14  profile_text_color                  11761 non-null  object\n",
      " 15  followers_count                     11761 non-null  int64 \n",
      " 16  location                            11761 non-null  object\n",
      " 17  listed_count                        11761 non-null  int64 \n",
      " 18  is_translation_enabled              11761 non-null  bool  \n",
      " 19  statuses_count                      11761 non-null  int64 \n",
      " 20  translator_type                     11761 non-null  object\n",
      " 21  friends_count                       11761 non-null  int64 \n",
      " 22  profile_link_color                  11761 non-null  object\n",
      " 23  profile_image_url                   11761 non-null  object\n",
      " 24  profile_background_image_url_https  11746 non-null  object\n",
      " 25  profile_background_color            11761 non-null  object\n",
      " 26  profile_banner_url                  11761 non-null  object\n",
      " 27  profile_background_image_url        11746 non-null  object\n",
      " 28  name                                11761 non-null  object\n",
      " 29  profile_background_tile             11761 non-null  bool  \n",
      " 30  favourites_count                    11761 non-null  int64 \n",
      " 31  screen_name                         11761 non-null  object\n",
      " 32  notifications                       11761 non-null  bool  \n",
      " 33  url                                 10299 non-null  object\n",
      " 34  time_zone                           11761 non-null  object\n",
      " 35  profile_sidebar_border_color        11761 non-null  object\n",
      " 36  default_profile                     11761 non-null  bool  \n",
      " 37  following                           11761 non-null  bool  \n",
      " 38  expanded_url                        11241 non-null  object\n",
      " 39  display_url                         11241 non-null  object\n",
      "dtypes: bool(9), int32(1), int64(9), object(21)\n",
      "memory usage: 2.8+ MB\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tzach\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:29:23] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { enable_categorical } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.89998\n",
      "[1]\tvalidation_0-auc:0.90777\n",
      "[2]\tvalidation_0-auc:0.91312\n",
      "[3]\tvalidation_0-auc:0.91998\n",
      "[4]\tvalidation_0-auc:0.92201\n",
      "[5]\tvalidation_0-auc:0.92424\n",
      "[6]\tvalidation_0-auc:0.92553\n",
      "[7]\tvalidation_0-auc:0.92589\n",
      "[8]\tvalidation_0-auc:0.92622\n",
      "[9]\tvalidation_0-auc:0.92882\n",
      "[10]\tvalidation_0-auc:0.93119\n",
      "[11]\tvalidation_0-auc:0.93080\n",
      "[12]\tvalidation_0-auc:0.93111\n",
      "[13]\tvalidation_0-auc:0.93303\n",
      "[14]\tvalidation_0-auc:0.93249\n",
      "[15]\tvalidation_0-auc:0.93355\n",
      "[16]\tvalidation_0-auc:0.93428\n",
      "[17]\tvalidation_0-auc:0.93377\n",
      "[18]\tvalidation_0-auc:0.93458\n",
      "[19]\tvalidation_0-auc:0.93492\n",
      "[20]\tvalidation_0-auc:0.93518\n",
      "[21]\tvalidation_0-auc:0.93536\n",
      "[22]\tvalidation_0-auc:0.93550\n",
      "[23]\tvalidation_0-auc:0.93595\n",
      "[24]\tvalidation_0-auc:0.93668\n",
      "[25]\tvalidation_0-auc:0.93686\n",
      "[26]\tvalidation_0-auc:0.93745\n",
      "[27]\tvalidation_0-auc:0.93746\n",
      "[28]\tvalidation_0-auc:0.93768\n",
      "[29]\tvalidation_0-auc:0.93751\n",
      "[30]\tvalidation_0-auc:0.93722\n",
      "[31]\tvalidation_0-auc:0.93740\n",
      "[32]\tvalidation_0-auc:0.93780\n",
      "[33]\tvalidation_0-auc:0.93820\n",
      "[34]\tvalidation_0-auc:0.93820\n",
      "[35]\tvalidation_0-auc:0.93843\n",
      "[36]\tvalidation_0-auc:0.93875\n",
      "[37]\tvalidation_0-auc:0.93841\n",
      "[38]\tvalidation_0-auc:0.93848\n",
      "[39]\tvalidation_0-auc:0.93885\n",
      "[40]\tvalidation_0-auc:0.93917\n",
      "[41]\tvalidation_0-auc:0.93917\n",
      "[42]\tvalidation_0-auc:0.93908\n",
      "[43]\tvalidation_0-auc:0.93937\n",
      "[44]\tvalidation_0-auc:0.93950\n",
      "[45]\tvalidation_0-auc:0.93952\n",
      "[46]\tvalidation_0-auc:0.93967\n",
      "[47]\tvalidation_0-auc:0.93968\n",
      "[48]\tvalidation_0-auc:0.93975\n",
      "[49]\tvalidation_0-auc:0.93976\n",
      "[50]\tvalidation_0-auc:0.93992\n",
      "[51]\tvalidation_0-auc:0.93957\n",
      "[52]\tvalidation_0-auc:0.93955\n",
      "[53]\tvalidation_0-auc:0.93919\n",
      "[54]\tvalidation_0-auc:0.93905\n",
      "[55]\tvalidation_0-auc:0.93854\n",
      "[56]\tvalidation_0-auc:0.93886\n",
      "[57]\tvalidation_0-auc:0.93880\n",
      "[58]\tvalidation_0-auc:0.93871\n",
      "[59]\tvalidation_0-auc:0.93884\n",
      "[60]\tvalidation_0-auc:0.93906\n",
      "[61]\tvalidation_0-auc:0.93926\n",
      "[62]\tvalidation_0-auc:0.93919\n",
      "[63]\tvalidation_0-auc:0.93932\n",
      "[64]\tvalidation_0-auc:0.93939\n",
      "[65]\tvalidation_0-auc:0.93958\n",
      "[66]\tvalidation_0-auc:0.93950\n",
      "[67]\tvalidation_0-auc:0.93920\n",
      "[68]\tvalidation_0-auc:0.93945\n",
      "[69]\tvalidation_0-auc:0.93948\n",
      "[70]\tvalidation_0-auc:0.93966\n",
      "[71]\tvalidation_0-auc:0.93957\n",
      "[72]\tvalidation_0-auc:0.93936\n",
      "[73]\tvalidation_0-auc:0.93924\n",
      "[74]\tvalidation_0-auc:0.93921\n",
      "[75]\tvalidation_0-auc:0.93925\n",
      "[76]\tvalidation_0-auc:0.93919\n",
      "[77]\tvalidation_0-auc:0.93914\n",
      "[78]\tvalidation_0-auc:0.93918\n",
      "[79]\tvalidation_0-auc:0.93907\n",
      "[80]\tvalidation_0-auc:0.93906\n",
      "[81]\tvalidation_0-auc:0.93923\n",
      "[82]\tvalidation_0-auc:0.93922\n",
      "[83]\tvalidation_0-auc:0.93884\n",
      "[84]\tvalidation_0-auc:0.93892\n",
      "[85]\tvalidation_0-auc:0.93887\n",
      "[86]\tvalidation_0-auc:0.93885\n",
      "[87]\tvalidation_0-auc:0.93941\n",
      "[88]\tvalidation_0-auc:0.93865\n",
      "[89]\tvalidation_0-auc:0.93861\n",
      "[90]\tvalidation_0-auc:0.93866\n",
      "[91]\tvalidation_0-auc:0.93836\n",
      "[92]\tvalidation_0-auc:0.93803\n",
      "[93]\tvalidation_0-auc:0.93793\n",
      "[94]\tvalidation_0-auc:0.93782\n",
      "[95]\tvalidation_0-auc:0.93768\n",
      "[96]\tvalidation_0-auc:0.93783\n",
      "[97]\tvalidation_0-auc:0.93764\n",
      "[98]\tvalidation_0-auc:0.93771\n",
      "[99]\tvalidation_0-auc:0.93767\n",
      "AUC score for the xgb metadata model(2015-2016):  85.40084388185653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tzach\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:29:27] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { enable_categorical } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-auc:0.73614\n",
      "[1]\tvalidation_0-auc:0.74677\n",
      "[2]\tvalidation_0-auc:0.74916\n",
      "[3]\tvalidation_0-auc:0.75819\n",
      "[4]\tvalidation_0-auc:0.76965\n",
      "[5]\tvalidation_0-auc:0.77358\n",
      "[6]\tvalidation_0-auc:0.78293\n",
      "[7]\tvalidation_0-auc:0.77978\n",
      "[8]\tvalidation_0-auc:0.78196\n",
      "[9]\tvalidation_0-auc:0.77814\n",
      "[10]\tvalidation_0-auc:0.78205\n",
      "[11]\tvalidation_0-auc:0.78518\n",
      "[12]\tvalidation_0-auc:0.79261\n",
      "[13]\tvalidation_0-auc:0.79504\n",
      "[14]\tvalidation_0-auc:0.79646\n",
      "[15]\tvalidation_0-auc:0.79823\n",
      "[16]\tvalidation_0-auc:0.79416\n",
      "[17]\tvalidation_0-auc:0.79580\n",
      "[18]\tvalidation_0-auc:0.79929\n",
      "[19]\tvalidation_0-auc:0.80191\n",
      "[20]\tvalidation_0-auc:0.80202\n",
      "[21]\tvalidation_0-auc:0.80155\n",
      "[22]\tvalidation_0-auc:0.80441\n",
      "[23]\tvalidation_0-auc:0.80386\n",
      "[24]\tvalidation_0-auc:0.80437\n",
      "[25]\tvalidation_0-auc:0.80588\n",
      "[26]\tvalidation_0-auc:0.80723\n",
      "[27]\tvalidation_0-auc:0.81043\n",
      "[28]\tvalidation_0-auc:0.80989\n",
      "[29]\tvalidation_0-auc:0.80802\n",
      "[30]\tvalidation_0-auc:0.80832\n",
      "[31]\tvalidation_0-auc:0.80861\n",
      "[32]\tvalidation_0-auc:0.80907\n",
      "[33]\tvalidation_0-auc:0.80867\n",
      "[34]\tvalidation_0-auc:0.80832\n",
      "[35]\tvalidation_0-auc:0.80788\n",
      "[36]\tvalidation_0-auc:0.80827\n",
      "[37]\tvalidation_0-auc:0.80801\n",
      "[38]\tvalidation_0-auc:0.80878\n",
      "[39]\tvalidation_0-auc:0.81052\n",
      "[40]\tvalidation_0-auc:0.81194\n",
      "[41]\tvalidation_0-auc:0.81210\n",
      "[42]\tvalidation_0-auc:0.81139\n",
      "[43]\tvalidation_0-auc:0.81196\n",
      "[44]\tvalidation_0-auc:0.81252\n",
      "[45]\tvalidation_0-auc:0.81264\n",
      "[46]\tvalidation_0-auc:0.81257\n",
      "[47]\tvalidation_0-auc:0.81257\n",
      "[48]\tvalidation_0-auc:0.81273\n",
      "[49]\tvalidation_0-auc:0.81216\n",
      "[50]\tvalidation_0-auc:0.81285\n",
      "[51]\tvalidation_0-auc:0.81251\n",
      "[52]\tvalidation_0-auc:0.81279\n",
      "[53]\tvalidation_0-auc:0.81301\n",
      "[54]\tvalidation_0-auc:0.81290\n",
      "[55]\tvalidation_0-auc:0.81385\n",
      "[56]\tvalidation_0-auc:0.81372\n",
      "[57]\tvalidation_0-auc:0.81398\n",
      "[58]\tvalidation_0-auc:0.81417\n",
      "[59]\tvalidation_0-auc:0.81438\n",
      "[60]\tvalidation_0-auc:0.81367\n",
      "[61]\tvalidation_0-auc:0.81443\n",
      "[62]\tvalidation_0-auc:0.81436\n",
      "[63]\tvalidation_0-auc:0.81392\n",
      "[64]\tvalidation_0-auc:0.81461\n",
      "[65]\tvalidation_0-auc:0.81467\n",
      "[66]\tvalidation_0-auc:0.81511\n",
      "[67]\tvalidation_0-auc:0.81578\n",
      "[68]\tvalidation_0-auc:0.81702\n",
      "[69]\tvalidation_0-auc:0.81774\n",
      "[70]\tvalidation_0-auc:0.81682\n",
      "[71]\tvalidation_0-auc:0.81702\n",
      "[72]\tvalidation_0-auc:0.81712\n",
      "[73]\tvalidation_0-auc:0.81791\n",
      "[74]\tvalidation_0-auc:0.81849\n",
      "[75]\tvalidation_0-auc:0.81897\n",
      "[76]\tvalidation_0-auc:0.81910\n",
      "[77]\tvalidation_0-auc:0.81947\n",
      "[78]\tvalidation_0-auc:0.81961\n",
      "[79]\tvalidation_0-auc:0.81947\n",
      "[80]\tvalidation_0-auc:0.81914\n",
      "[81]\tvalidation_0-auc:0.82002\n",
      "[82]\tvalidation_0-auc:0.82022\n",
      "[83]\tvalidation_0-auc:0.82017\n",
      "[84]\tvalidation_0-auc:0.82027\n",
      "[85]\tvalidation_0-auc:0.82057\n",
      "[86]\tvalidation_0-auc:0.82150\n",
      "[87]\tvalidation_0-auc:0.82098\n",
      "[88]\tvalidation_0-auc:0.82083\n",
      "[89]\tvalidation_0-auc:0.82103\n",
      "[90]\tvalidation_0-auc:0.82162\n",
      "[91]\tvalidation_0-auc:0.82163\n",
      "[92]\tvalidation_0-auc:0.82180\n",
      "[93]\tvalidation_0-auc:0.82213\n",
      "[94]\tvalidation_0-auc:0.82178\n",
      "[95]\tvalidation_0-auc:0.82212\n",
      "[96]\tvalidation_0-auc:0.82258\n",
      "[97]\tvalidation_0-auc:0.82282\n",
      "[98]\tvalidation_0-auc:0.82387\n",
      "[99]\tvalidation_0-auc:0.82404\n",
      "AUC score for the xgb text model(2015-2016):  73.33333333333333\n",
      "The classifier found that Trump wrote 1580 tweets from the other devices while the unknown assistant wrote 797 tweets from the other devices\n",
      "Dataset and tweets*before* preprocessing and text normalization:\n",
      " 0    NaN\n",
      "1    NaN\n",
      "2    NaN\n",
      "3    NaN\n",
      "4    NaN\n",
      "Name: text, dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2605 entries, 0 to 2604\n",
      "Data columns (total 31 columns):\n",
      " #   Column                     Non-Null Count  Dtype              \n",
      "---  ------                     --------------  -----              \n",
      " 0   favorited                  2605 non-null   bool               \n",
      " 1   contributors               0 non-null      float64            \n",
      " 2   truncated                  2605 non-null   bool               \n",
      " 3   is_quote_status            2605 non-null   bool               \n",
      " 4   in_reply_to_status_id      45 non-null     float64            \n",
      " 5   user                       2605 non-null   object             \n",
      " 6   geo                        0 non-null      float64            \n",
      " 7   id                         2605 non-null   int64              \n",
      " 8   favorite_count             2605 non-null   int64              \n",
      " 9   lang                       2605 non-null   object             \n",
      " 10  full_text                  1554 non-null   object             \n",
      " 11  entities                   2605 non-null   object             \n",
      " 12  created_at                 2605 non-null   datetime64[ns, UTC]\n",
      " 13  retweeted                  2605 non-null   bool               \n",
      " 14  coordinates                0 non-null      float64            \n",
      " 15  in_reply_to_user_id_str    46 non-null     float64            \n",
      " 16  source                     2605 non-null   object             \n",
      " 17  in_reply_to_status_id_str  45 non-null     float64            \n",
      " 18  in_reply_to_screen_name    46 non-null     object             \n",
      " 19  in_reply_to_user_id        46 non-null     float64            \n",
      " 20  display_text_range         1554 non-null   object             \n",
      " 21  place                      50 non-null     object             \n",
      " 22  retweet_count              2605 non-null   int64              \n",
      " 23  id_str                     2605 non-null   int64              \n",
      " 24  possibly_sensitive         847 non-null    float64            \n",
      " 25  extended_entities          497 non-null    object             \n",
      " 26  quoted_status_id_str       87 non-null     float64            \n",
      " 27  quoted_status_id           87 non-null     float64            \n",
      " 28  quoted_status              66 non-null     object             \n",
      " 29  retweeted_status           310 non-null    object             \n",
      " 30  text                       1051 non-null   object             \n",
      "dtypes: bool(4), datetime64[ns, UTC](1), float64(10), int64(4), object(12)\n",
      "memory usage: 559.8+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clean_text_progress: 100%|████████████████████████████████████████████████████████| 2605/2605 [00:09<00:00, 270.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset and tweets *after* preprocessing and text normalization:\n",
      " 0    [leave, florida, washington, c, today, p, much...\n",
      "1    [iran, fail, every, level, despite, terrible, ...\n",
      "2    [unite, state, foolishly, give, pakistan, bill...\n",
      "3    [happy, new, year, make, america, great, much,...\n",
      "4    [country, rapidly, grow, stronger, smarter, wa...\n",
      "Name: text, dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2605 entries, 0 to 2604\n",
      "Data columns (total 20 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   truncated                2605 non-null   bool  \n",
      " 1   is_quote_status          2605 non-null   bool  \n",
      " 2   favorite_count           2605 non-null   int64 \n",
      " 3   lang                     2605 non-null   object\n",
      " 4   source                   2605 non-null   int32 \n",
      " 5   retweet_count            2605 non-null   int64 \n",
      " 6   text                     2605 non-null   object\n",
      " 7   day                      2605 non-null   int64 \n",
      " 8   hour                     2605 non-null   int64 \n",
      " 9   profile_image_url_https  2605 non-null   object\n",
      " 10  followers_count          2605 non-null   int64 \n",
      " 11  listed_count             2605 non-null   int64 \n",
      " 12  utc_offset               2605 non-null   int64 \n",
      " 13  statuses_count           2605 non-null   int64 \n",
      " 14  description              2605 non-null   object\n",
      " 15  friends_count            2605 non-null   int64 \n",
      " 16  profile_link_color       2605 non-null   object\n",
      " 17  profile_image_url        2605 non-null   object\n",
      " 18  profile_banner_url       2605 non-null   object\n",
      " 19  favourites_count         2605 non-null   int64 \n",
      "dtypes: bool(2), int32(1), int64(10), object(7)\n",
      "memory usage: 361.4+ KB\n",
      "The classifier found that Trump wrote 1442 tweets in 2018  while the unknown assistant wrote 1163 tweets in 2018\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    df = load_data() #2015-2016 tweets\n",
    "    df = preprocess_old_tweets(df) #preprocess the data\n",
    "    X_train, X_test, y_train, y_test, x_other, y_other, tfv, train_sm, test_sm = tfidf(df)      #tfidf fitting\n",
    "    xgboost, metadata_score = xgb_model(X_train, X_test, y_train, y_test)   #model based on all the data\n",
    "    print(\"AUC score for the xgb metadata model(2015-2016): \", metadata_score)\n",
    "    xgboost_text_model, text_score = xgb_model(train_sm, test_sm, y_train, y_test)      #model based only on the text using tfidf\n",
    "    print(\"AUC score for the xgb text model(2015-2016): \", text_score)\n",
    "    model_2016(x_other, xgboost)\n",
    "\n",
    "    df2 = pd.read_json(r'master_2018')      #2018 tweets\n",
    "    df2 = preprocess_2018_tweets(df2)       #preprocess_2018_tweets we found that they features have changed since 2016\n",
    "                                            #so the metadata model cant be used, only model based on text.\n",
    "    model_2018(df2, xgboost_text_model, tfv)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "optional-huntington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Our classifier was designed to classify only between tweets written from the android and the iPhone cellphones.\\n\\nThus, the train data didn't include the other devices. We can say that the chances are good when considering the text and metadata.\\n\\nHowever, in 2018 the tweets data was added with new features and besides the time that passed leads to data drift. We can't technically use the old model because of the added features, so I used a model trained only on the text of the 2015-2016 tweets. relying on it we can use the model, but the accuracy is low even before we take into account the data drift over time.\\n\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    ''' Our classifier was designed to classify only between tweets written from the android and the iPhone cellphones.\n",
    "\n",
    "Thus, the train data didn't include the other devices. We can say that the chances are good when considering the text and metadata.\n",
    "\n",
    "However, in 2018 the tweets data was added with new features and besides the time that passed leads to data drift. We can't technically use the old model because of the added features, so I used a model trained only on the text of the 2015-2016 tweets. relying on it we can use the model, but the accuracy is low even before we take into account the data drift over time.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "authentic-breach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please describe your process (either in Jupyter notebook, or in separate file):\\n1. What approaches did you try? Which worked and which failed?\\n2. What algorithms did you use? How did you evaluate performance?\\n\\n1. My approach was using tfidf from the start, if I could get a good AUC score, I thought about removing some of the features or all the metadata but the score was best when I had used all of the features.\\nAt first I also tried to use all the data without balancing it, as it was 1:3 ratio +- and I got a better score in the test, but I felt as it might be data leakage even when I use AOC.\\nI also tried to change the evaluation for logloss for example, but it didnt effect the score, so I left the AOC. \\nGoing only by the text also failed, approach like that should be used maybe with a DNN to better classify the writer.\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''
    "1. My approach was using tfidf from the start, if I could get a good AUC score, I thought about removing some of the features or all the metadata but the score was best when I had used all of the features.\n",
    "At first I also tried to use all the data without balancing it, as it was 1:3 ratio +- and I got a better score in the test, but I felt as it might be data leakage even when I use AOC.\n",
    "I also tried to change the evaluation for logloss for example, but it didnt effect the score, so I left the AOC. \n",
    "Going only by the text also failed, approach like that should be used maybe with a DNN to better classify the writer.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-complaint",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
